<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.80.0" />


<title>Equilibrium in the Besley-Case Model - Bas Machielsen</title>
<meta property="og:title" content="Equilibrium in the Besley-Case Model - Bas Machielsen">


  <link href='https://bas-m.netlify.app/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="https://bas-m.netlify.app/css/fonts.css" media="all">
<link rel="stylesheet" href="https://bas-m.netlify.app/css/main.css" media="all">


<link rel="icon" href="https://bas-m.netlify.app/img/favicon.ico" type="image/x-icon">

  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="https://bas-m.netlify.app/" class="nav-logo">
    <img src="https://bas-m.netlify.app/images/bas_m.png"
         width="60"
         height="60"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="https://bas-m.netlify.app/">Home</a></li>
    
    <li><a href="https://bas-m.netlify.app/blogs/">Blog</a></li>
    
    <li><a href="https://bas-m.netlify.app/resume/">Resume</a></li>
    
    <li><a href="https://bas-m.netlify.app/classes/">Classes</a></li>
    
    <li><a href="https://bas-m.netlify.app/packages/">R Packages</a></li>
    
    <li><a href="https://bas-m.netlify.app/papers/">Papers</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">10 min read</span>
    

    <h1 class="article-title">Equilibrium in the Besley-Case Model</h1>

    
    <span class="article-date">2022-05-19</span>
    

    <div class="article-content">
      


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Besley and Case (1995) aahave a paper about the influence of term limits for politicians on policy behavior. They use a dynamic game. Politicians can serve either 1, or 2, periods, depending on whether they are retained. In these periods, politicians choose to put in effort. Putting in more or less effort is costly to different extents, according to their type. The chosen effort probabilistically affects a policy outcome. An infinitely-lived voter makes decisions to retain or dismiss the politician after observing the outcome. Their solution, based on Banks and Sundaram (1998) involves a <em>cut-off equilibrium</em>, with voters reelecting an incumbent policy if the probabilistically-influenced policy variable realization is higher than some threshold <span class="math inline">\(r_{min}\)</span>.</p>
<p>One of their predictions is that the equilibrium amount of effort put in by the politicians who are in their first period is higher than in their last period, irrespective of the politician’s type. Also, politicians who are in their second period are more likely to be high-type politicians, so that cross-sectionally, it seems that politicians in the second period have put in more effort than politicians in the first period, even though, if one looks at a “panel” of politicians over two periods, the first period effort is higher than the second period. The interpretation is that the policy outcome is better when there is politicians who do not face a term limit, irrespective of the type (ability) of the politicians. As they put it:</p>
<blockquote>
<p>If two terms are allowed, then incumbents who give higher first-term payoffs to voters are more likely to be retained to serve a second term. Those in their last term put in less effort and give lower payoffs to voters, on average, compared with their first term in office.</p>
</blockquote>
<p>Their model is fairly general, so in this blog post, I wanted to give an easier to understand version for a specific case using Python to solve for some expressions that are not analytically soluble. I also want to demonstrate how to use standard computational techniques in macro for this specific case.</p>
</div>
<div id="set-up" class="section level2">
<h2>Set-up</h2>
<p>In the Besley and Case (1995) model, there can be <span class="math inline">\(k\)</span> types of politicians, ordered by ability <span class="math inline">\(\omega_1 &lt; \dots &lt; \omega_k\)</span>. They occur with probabilities <span class="math inline">\(\pi_1, \dots, \pi_k\)</span>. In my setup, there are only two types, <span class="math inline">\(\omega_1 &lt; \omega_2\)</span> with <span class="math inline">\(\omega_1 = 3\)</span> and <span class="math inline">\(\omega_2 = 9\)</span>. <span class="math inline">\(\omega_1\)</span> occurs with probability <span class="math inline">\(\pi_1 = 0.2\)</span> and <span class="math inline">\(\omega_2\)</span> with probability <span class="math inline">\(\pi_2 = 0.8\)</span>. The politicians per-period utility is:</p>
<p><span class="math display">\[
v(\alpha, \omega_i) = \alpha - \frac{\alpha^2}{\omega_i}
\]</span>
If the politician is in power, otherwise, utility is zero. This reflects an expected reward proportional to effort, and the utility is negative in effort, meaning that effort is costly, but less negative for higher-quality types.</p>
<p>Effort is mapped into policy outcomes, <span class="math inline">\(r\)</span>, probabilistically via <span class="math inline">\(F(\alpha, r)\)</span>. In the Besley and Coate (1995) setting, this can be any distribution satisfying the monotone likelihood ratio property. In my setting, I take <span class="math inline">\(F(\alpha, r)\)</span> to be <span class="math inline">\(\Phi(\alpha)\)</span>, meaning <span class="math inline">\(R \sim \mathcal{N}(\alpha, 1)\)</span>. The voter’s one-period utility is simply the realized outcome <span class="math inline">\(r\)</span>.</p>
</div>
<div id="optimization-of-the-politician" class="section level2">
<h2>Optimization of the politician</h2>
<p>In the second period, when retained, the politician will simply optimize their one-period utility:</p>
<p><span class="math display">\[
a^s_i = \frac{w_i}{4}
\]</span>
In the first period, the politician will optimize:</p>
<p><span class="math display">\[
a^l_i = {\arg \max} \left\{ v(\alpha, \omega_i) + \delta \cdot [\text{Pr}(r \in R(\sigma) | \alpha) \cdot v(a_s (\omega_i), \omega_i)] \right\}
\]</span>
In words, this means the politician will take into account their second-period optimal strategy, and the likelihood that their realized <span class="math inline">\(r\)</span> is greater than the voter’s retention <span class="math inline">\(r\)</span>. Given a <em>retention rule</em>, that probability is further specified as Pr(<span class="math inline">\(r &gt; r_{min}\)</span>).</p>
</div>
<div id="optimization-of-the-voter" class="section level2">
<h2>Optimization of the voter</h2>
<p>After observing the realized <span class="math inline">\(r\)</span> after one period, the voter computes the belief that the incumbent is of type 1 (the low type) as follows:</p>
<p><span class="math display">\[
\beta_1 (r) = \frac{\pi_1 \cdot f(r; a_l (\omega_1))}{\pi_1 \cdot f(r; a_l (\omega_1)) + \pi_2 \cdot f(r; a_l(\omega_2))}
\]</span>
The expected utility to the voter of retention is then:</p>
<p><span class="math display">\[
v_2 (r, \alpha^s, \alpha^l) = \sum_{i=1}^2 \int \beta_i (r) v(\hat{r})f(\hat{r}|\alpha^s_i) d\hat{r}
\]</span>
which corresponds to the probability that the player is of type <span class="math inline">\(i\)</span>, and then integrating over the entire range of potential <span class="math inline">\(r\)</span>s, weighted by the density, given that they put in the optimal second period effort corresponding to their type.</p>
<p>Banks and Sundaram (1993) then show that the Bellman equation for the voter’s problem is:</p>
<p><span class="math display">\[
W(1, r, p) = \max \left\{v_2 (r, \alpha^s, \alpha^l) + \alpha W(0, r, \pi), W(0, r, \pi) \right\}
\]</span>
where the state vector consists of <span class="math inline">\(\{ 0, 1\}\)</span>, meaning how many periods the politician can still be retained, the realized <span class="math inline">\(r\)</span> in the period, and the beliefs of the voter regarding the incumbent type <span class="math inline">\(p\)</span>. In general, the last element of the tripe <span class="math inline">\((\{0,1\}, r, p)\)</span> is the <span class="math inline">\(n\)</span>-dimensional simplex, but because there are only 2 types, it is now summarized by <span class="math inline">\(\beta_1 (r)\)</span>.</p>
<p>The Bellman equation for the state <span class="math inline">\(W(0,r,p)\)</span> is equal to the expected immediate reward plus the probability-weighted discounted future reward for each <span class="math inline">\(W(1,r,p)\)</span>:</p>
<p><span class="math display">\[
W(0, r, p) = \sum_{i=1}^2 \pi_i \cdot \int \hat{r} f(\hat{r}|a^s_i) d\hat{r} + \delta \int W(1, \hat{r}, \beta_1 (\hat{r})) \cdot \sum_{k=1}^2 \pi_k \cdot f(\hat{r} | \alpha^s_k) d \hat{r}
\]</span>
Now that we have the Bellman, we can use value-function iteration to derive the optimal <span class="math inline">\(r\)</span>. We also need to take into account equilibrium conditions, so that (i) the voters respond to the optimal retention <span class="math inline">\(r\)</span> according to the value function, and (ii) the politicians always set the best retention <span class="math inline">\(r\)</span> given a particular estimate of the value function. Sundaram and Banks (1993) also prove that the dynamic programming theorems for this to be a contraction mapping hold.</p>
</div>
<div id="implementation" class="section level2">
<h2>Implementation</h2>
<p>Below, I load the libraries, and I store a Python class called <code>RetentionModel</code>.</p>
<pre class="python"><code>#%matplotlib inline
import matplotlib.pyplot as plt
plt.rcParams[&quot;figure.figsize&quot;] = (11, 5)  #set default figure size
import numpy as np
from interpolation import interp
from scipy.optimize import minimize_scalar, bisect
from scipy.stats import norm
import scipy.integrate as integrate
import scipy.interpolate as interpolate

from numba import jit, njit, prange, float64, int32
from numba.experimental import jitclass

from tqdm import tqdm</code></pre>
<pre class="python"><code>class RetentionModel:
  
    def __init__(self,delta=0.95,alpha=0.2,types=(3, 9),types_freq=(0.2, 0.8),up=6,low=2):
        self.delta, self.alpha, self.types, self.types_freq, self.up, self.low = delta, alpha, types, types_freq, up, low
    
        #&#39;create a 3-dimensional grid&#39;
        #&#39;optimal r probably somewhere in between the means&#39; of the efforts
        self.x_grid = np.mgrid[0:2, 2.5:3.5:0.1, 0:1.05:0.1].reshape(3, -1).T
        
    def evaluated_int(self, draws, value_f, typ, retention_r):
        &quot;&quot;&quot;Evaluate an integral&quot;&quot;&quot;
        beta_1 = self.beta_1
        types_freq = self.types_freq
        
        n = len(draws)
        realization = [value_f([1, i, beta_1(i, retention_r)])[0] for i in draws]
        samples = np.dot(draws, realization)
    
        return types_freq[typ]*np.mean(realization)
  
    def vot_util(self, x):
        &quot;&quot;&quot;voter utility function&quot;&quot;&quot;
        return x

    def f(self, r, effort):
        &quot;&quot;&quot;normal pdf of r according to exerted effort&quot;&quot;&quot;
        return norm.pdf(r, loc = effort, scale = 1)
    
    def F(self, r, effort):
        &quot;&quot;&quot;normal cdf of r according to exerted effort&quot;&quot;&quot;
        return norm.cdf(r, loc = effort, scale = 1)

    def expected_utility(self,effort, typ, retention_r, delta):
        F = self.F
        delta = self.delta
        &quot;&quot;&quot;
        Return the total expected utility a priori from 
        the second period and then from the first period 
        as a function of the retention r, discount rate and exterted effort
        &quot;&quot;&quot;
        out = effort - (effort**2)/typ + delta*(1-F(retention_r, effort))*typ/4
        return out

    def effort_first_period(self, typ, retention_r):
        &quot;&quot;&quot;optimal effort in the first period given type&quot;&quot;&quot;
        #changed upper limit to typ, from 5
        return maximize(self.expected_utility, 1e-15, typ, (typ, retention_r, self.delta))[0]

    def effort_second_period(self, typ):
        &quot;&quot;&quot;optimal effort in the second period given type and realized r&quot;&quot;&quot;
        return typ/2
    
    def beta_1(self, r, retention_r):
        &quot;&quot;&quot;
        probability that type = type_1 after observing a specific r
        &quot;&quot;&quot;
        f = self.f
        types = self.types
        types_freq = self.types_freq
        
        num = types_freq[0]*f(r, self.effort_first_period(types[0], retention_r))
        denom = num + types_freq[1]*f(r, self.effort_first_period(types[1], retention_r))
        
        return num/denom

    def v_2(self, r, retention_r, eff_sp_t1, eff_sp_t2):
        &quot;&quot;&quot;
        Calculate the second period expected utility given a certain realized r
        and the optimal efforts by the players
        &quot;&quot;&quot;
        types, types_freq, f, vot_util = self.types, self.types_freq, self.f, self.vot_util
        beta_1 = self.beta_1

        def integrand(x, e):
            return vot_util(x)*f(x, e)
            
        #&#39;under linear utility, this just simplifies to the expected value&#39;
        out1 = beta_1(r, retention_r)*integrate.quad(integrand, a=-np.inf, b=np.inf, args=eff_sp_t1)[0]
        out2 = (1-beta_1(r, retention_r))*integrate.quad(integrand, a=-np.inf, b=np.inf, args=eff_sp_t2)[0]
        
        return out1 + out2
    
    
    def state_action_value(self, action, state, v_array, retention_r):
        &quot;&quot;&quot;
        Given state x_grid=[{0,1}, r, beliefs], and choice actions (k,d) and a guess v,
        what is the maximum (RHS bellman equation)
        &quot;&quot;&quot;
        v_2, types, types_freq, beta_1, delta, alpha, f = self.v_2, self.types, self.types_freq, self.beta_1, self.delta, self.alpha, self.f
        vot_util, effort_first_period = self.vot_util, self.effort_first_period
        evaluated_int = self.evaluated_int
        
        #a multidimensional linear interpolator - make v a function 
        v = interpolate.LinearNDInterpolator(self.x_grid, v_array, fill_value=0)
        
        if state[0] == 0:            
        # return is eq. to instantaneous reward + expectation over all future 1 states 
        
            out1 = types_freq[0]*effort_first_period(types[0], retention_r)
            out2 = types_freq[1]*effort_first_period(types[1], retention_r)
            
            def integrand(x, *args): #v, beta_1, types, types_freq, effort_first_period, retention_r):
                f = self.f
                
                term1 = types_freq[0]*f(x, effort_first_period(types[0], retention_r))
                term2 = types_freq[1]*f(x, effort_first_period(types[1], retention_r))
                out = v([1, x, beta_1(x, retention_r)])[0]*(term1 + term2)
                
                return out
            
            def calc_integral(func, a, b, n, *args):
                delta = (b-a)/n
                points = np.linspace(a, b, n)
                
                values = [func(i, *args) for i in points]
                #values = func(points, *args)
    
                return delta*np.sum(values)

            
            out3 = calc_integral(integrand, 0, 13, 30, 
                                 v, 
                                 beta_1, 
                                 types, 
                                 types_freq, 
                                 effort_first_period,
                                 retention_r)
                                  
            return out1 + out2 + alpha*(out3)
        
        elif state[0] == 1:
        #both dismiss and keep are possible    
            if action == &quot;k&quot;:
                one = alpha*v([0, state[1], types_freq[0]])[0]
                two = v_2(state[1], 
                          retention_r, 
                          self.effort_second_period(types[0]), 
                          self.effort_second_period(types[1]))
                
                return one + two 
            
            elif action == &quot;d&quot;:
                return v([0, state[1], types_freq[0]])[0]
            
            else:
                raise ValueError(&quot;No action chosen&quot;)
        </code></pre>
<p>I also define a custom maximizer, to take into account that the voter maximizes over a dichotomous <span class="math inline">\(\{\)</span>dismiss, retain<span class="math inline">\(\}\)</span> decision. The politician uses a normal (continuous) maximizer, which I also implement. Finally, I use a Monte Carlo approximation for most complicated integrals I calculate:</p>
<pre class="python"><code>def maximize(g, a, b, args):
    &quot;&quot;&quot;
    Maximize the function g over the interval [a, b].

    We use the fact that the maximizer of g on any interval is
    also the minimizer of -g.  The tuple args collects any extra
    arguments to g.

    Returns the maximal value and the maximizer.
    &quot;&quot;&quot;

    objective = lambda x: -g(x, *args)
    result = minimize_scalar(objective, bounds=(a, b), method=&#39;bounded&#39;)
    maximizer, maximum = result.x, -result.fun
    return maximizer, maximum
  
# Write a maximizer like the concept maximizer above to find the optimal value for each point on the grid

def maximize_sav(function, args):
    &quot;&quot;&quot;
    a function that maximizes the sav for a given state, v_array, and retention_r
    and on the basis of choices k,v
    
    returns the maximizer (the best action) and the maximum
    &quot;&quot;&quot;
    state = args[0]
    v_array = args[1]
    retention_r = args[2]
    
    if state[0] == 0:
        action = &quot;d&quot;
        maximum = function(&quot;d&quot;, state, v_array, retention_r)
        
    elif state[0] == 1:
        
        dismiss = function(&quot;d&quot;, state, v_array, retention_r)
        keep = function(&quot;k&quot;, state, v_array, retention_r)
        if  dismiss &gt; keep:
            action = &quot;d&quot;
            maximum = dismiss
        else:
            action = &quot;k&quot;
            maximum = keep
        
    return action, maximum


def monte_carlo_uniform(func,
                       a=-10,
                       b=10,
                       n=150):
    
    subsets = np.arange(0, n+1, n/10)
    steps = n/10
    u = np.zeros(n)
    for i in range(10):
        start = int(subsets[i])
        end = int(subsets[i+1])
        u[start:end] = np.random.uniform(low=i/10, high=(i+1)/10, size=end-start)
    np.random.shuffle(u)
    u_func = func(a+(b-a)*u)
    s = ((b-a)/n)*u_func.sum()
    
    return s</code></pre>
</div>
<div id="value-function-iteration" class="section level2">
<h2>Value Function Iteration</h2>
<p>Then, I instantiate an instance of the model and apply value function iteration:</p>
<pre class="python"><code>model = RetentionModel(delta=0.99, alpha = 0.2)</code></pre>
<pre class="python"><code>def T(v, rm, retention_r):
    &quot;&quot;&quot;
    The Bellman operator.  Updates the guess of the value function.

    * ce is an instance of CakeEating
    * v is an array representing a guess of the value function

    &quot;&quot;&quot;
    v_new = np.empty_like(v)
    p_new = [None]*len(v_new)

    for i, x in enumerate(rm.x_grid):
        # Maximize RHS of Bellman equation at state x
        #replace by my custom maximize function
        sol = maximize_sav(rm.state_action_value, args=(x, v, retention_r))
        p_new[i] = sol[0]
        v_new[i] = sol[1]
        
    new_retention_r = rm.x_grid[p_new.index(&#39;k&#39;)][1]

    return p_new, v_new, new_retention_r</code></pre>
<pre class="python"><code>def compute_value_function(rm,
                           tol=1e-3,
                           max_iter=200,
                           verbose=True,
                           print_skip=1):

    # Set up loop
    v = np.zeros(len(rm.x_grid)) # Initial guess
    rr = 2 # Initial guess
    
    i = 0
    error = tol + 1

    while i &lt; max_iter and error &gt; tol:
        update = T(v, rm, rr)
        
        p_new = update[0]
        v_new = update[1]
        rr_new = update[2]
        
        error = np.max(np.abs(v - v_new))
        i += 1

        if verbose and i % print_skip == 0:
            print(f&quot;Error at iteration {i} is {error}.&quot;)

        v = v_new
        rr = rr_new

    if i == max_iter:
        print(&quot;Failed to converge!&quot;)

    if verbose and i &lt; max_iter:
        print(f&quot;\nConverged in {i} iterations.&quot;)

    return p_new, v_new, rr_new</code></pre>
</div>
<div id="solution" class="section level2">
<h2>Solution</h2>
<p>The procedure converges and the model is saved under <code>out</code>:</p>
<pre class="python"><code>out = compute_value_function(model)</code></pre>
<pre><code>## Error at iteration 1 is 4.09024743100576.
## Error at iteration 2 is 1.608588398221574.
## Error at iteration 3 is 0.17448573956574087.
## Error at iteration 4 is 0.14479551187642237.
## Error at iteration 5 is 0.06623950959972902.
## Error at iteration 6 is 0.03690126342482625.
## Error at iteration 7 is 0.03690126342482625.
## Error at iteration 8 is 0.0003930360580230996.
## 
## Converged in 8 iterations.</code></pre>
<p>and the optimal retention <span class="math inline">\(r\)</span> for this specific case is:</p>
<pre class="python"><code>out[2]</code></pre>
<pre><code>## 3.3</code></pre>
</div>

    </div>
  </article>

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//http-bas-m-netlify-app.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>



</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="https://bas-m.netlify.app/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="https://bas-m.netlify.app/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="https://bas-m.netlify.app/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

