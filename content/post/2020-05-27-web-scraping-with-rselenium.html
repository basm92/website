---
title: Web Scraping with RSelenium
author: ~
date: '2020-05-27'
slug: web-scraping-with-rselenium
categories: []
tags: []
---



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>In this blogpost, I want to tell you about web scraping using <code>RSelenium</code>, something which I’ve recently been learning and which I am very enthusiastic about, because it is very powerful. I want to do this in the context of an example. I have been supervising undergraduate students, and a group of students asked me where they could get a list of members of the Iranian parliament. So I went to <a href="https://en.parliran.ir/eng/en/10th%20Term">the official English-language website</a> of the Iranian parliament, and looked up the list of members.</p>
<p>Naturally, I wanted to scrape all of these names. For those of you familiar with scraping, a first approach would be something like the following. For those who aren’t, I have another nice blog post about elementary web scraping, but maybe you can follow along!</p>
<ul>
<li><p>First, we observe, by looking at the html-structure of the page, that all the names are stored in a <code>&lt;div&gt;</code> of class <code>text-left h4</code>.</p></li>
<li><p>We can use <a href="https://www.w3schools.com/css/css_selectors.asp">CSS selectors</a> to find the specific elements of these classes:</p></li>
</ul>
<pre class="r"><code>library(rvest)
url &lt;- &quot;https://en.parliran.ir/eng/en/10th%20Term&quot;

read_html(url) %&gt;%
  html_nodes(&quot;div .text-left.h4:not(.blue)&quot;)</code></pre>
<pre><code>## {xml_nodeset (10)}
##  [1] &lt;div class=&quot;text-left h4&quot;&gt;\n&lt;strong&gt;Name:&lt;/strong&gt;Abasi, Asadollah&lt;/div&gt;
##  [2] &lt;div class=&quot;text-left h4&quot;&gt;\n&lt;strong&gt;Name:&lt;/strong&gt;Abedi, Heydar Ali&lt;/div&gt;
##  [3] &lt;div class=&quot;text-left h4&quot;&gt;\n&lt;strong&gt;Name:&lt;/strong&gt;Abramian, Jorjik&lt;/div&gt;
##  [4] &lt;div class=&quot;text-left h4&quot;&gt;\n&lt;strong&gt;Name:&lt;/strong&gt;Abtahi, seyed Mohammad ...
##  [5] &lt;div class=&quot;text-left h4&quot;&gt;\n&lt;strong&gt;Name:&lt;/strong&gt;Abutorabi, Abolfazl&lt;/div&gt;
##  [6] &lt;div class=&quot;text-left h4&quot;&gt;\n&lt;strong&gt;Name:&lt;/strong&gt;Adyani rad, Seyed Ali&lt; ...
##  [7] &lt;div class=&quot;text-left h4&quot;&gt;\n&lt;strong&gt;Name:&lt;/strong&gt;Afzali, Nazar&lt;/div&gt;
##  [8] &lt;div class=&quot;text-left h4&quot;&gt;\n&lt;strong&gt;Name:&lt;/strong&gt;Afzali, Seyed Hossein&lt; ...
##  [9] &lt;div class=&quot;text-left h4&quot;&gt;\n&lt;strong&gt;Name:&lt;/strong&gt;Aghapour Alishahi, Mas ...
## [10] &lt;div class=&quot;text-left h4&quot;&gt;\n&lt;strong&gt;Name:&lt;/strong&gt;Ahmadi Lashki, Qasem&lt;/ ...</code></pre>
<p>Here I am saying, select all nodes (separate chunks of html, be it <code>&lt;p&gt;</code>, <code>&lt;a&gt;</code>, <code>&lt;div&gt;</code> or something else), that are <code>&lt;div&gt;</code>, and have the classes <code>text-left</code>, <code>h4</code>, and don’t have <code>blue</code>. (The reason for no blue is that it would take some unnecessary information. Try it out yourself!)</p>
<p>Now, we can extract the names with the help of the <code>html_text</code> command from the <code>rvest</code> package, which extract all text inside these particular html-elements:</p>
<pre class="r"><code>read_html(url) %&gt;%
  html_nodes(&quot;div .text-left.h4:not(.blue)&quot;) %&gt;%
  html_text()</code></pre>
<pre><code>##  [1] &quot;Name:Abasi, Asadollah&quot;             &quot;Name:Abedi, Heydar Ali&quot;           
##  [3] &quot;Name:Abramian, Jorjik&quot;             &quot;Name:Abtahi, seyed Mohammad Javad&quot;
##  [5] &quot;Name:Abutorabi, Abolfazl&quot;          &quot;Name:Adyani rad, Seyed Ali&quot;       
##  [7] &quot;Name:Afzali, Nazar&quot;                &quot;Name:Afzali, Seyed Hossein&quot;       
##  [9] &quot;Name:Aghapour Alishahi, Masoumeh&quot;  &quot;Name:Ahmadi Lashki, Qasem&quot;</code></pre>
<p>Don’t worry about the <code>Name:</code> parts for now. We’ll clean those later.</p>
<p>The difficulty is: these are obviously not all members of Iran’s parliament. If you go to the <a href="https://en.parliran.ir/eng/en/10th%20Term">actual website</a>, you will notice that there are pages 1 to 10 (and up to 29), which you can click. But if you do so, the page’s URL does not change! That means we cannot readily scrape the names of all members by navigating to a new URL. Here, <code>RSelenium</code> comes into play.</p>
</div>
<div id="rselenium" class="section level2">
<h2>RSelenium</h2>
<ul>
<li>What is Selenium?</li>
</ul>
<blockquote>
<p>Selenium is a project focused on automating web browsers.</p>
</blockquote>
<p>from <a href="https://cran.r-project.org/web/packages/RSelenium/vignettes/basics.html">this vignette</a>. In layman’s terms, Selenium creates ‘robot browsers’ that you can control by giving commands.</p>
<ul>
<li>What is RSelenium?</li>
</ul>
<blockquote>
<p>The goal of RSelenium is to make it easy to connect to a Selenium Server/Remote Selenium Server from within R. RSelenium provides R bindings for the Selenium Webdriver API.</p>
</blockquote>
<blockquote>
<p>RSelenium allows you to carry out unit testing and regression testing on your webapps and webpages across a range of browser/OS combinations. This allows us to integrate from within R testing and manipulation of popular projects such as Shiny Apps.</p>
</blockquote>
<p>Originally, RSelenium was used primarily to test apps by creating virtual environments of different browsers, and see how they look/behave. We will be using it to create a virtual browser, give it some commands, and extract data in a <code>for</code> loop as we go. Specifically, we will use the virtual browser to navigate through each of the 10 pages, and scrape all the names as we go along!</p>
</div>
<div id="setting-up-a-virtual-browser" class="section level2">
<h2>Setting up a Virtual Browser</h2>
<p>To get <code>RSelenium</code> working, we need two things: <a href="https://www.docker.com/why-docker">Docker</a> to set up a server to run the virtual browser on, and the <code>RSelenium</code> package.</p>
<p>In Ubuntu, the OS of my preference, Docker can be installed in the following way:</p>
<pre><code>$ sudo apt-get install docker.io</code></pre>
<p>.. and you start it, up and test whether it is working by executing the following query in the command bash:</p>
<pre><code>$ sudo service docker start

$ sudo docker run hello-world</code></pre>
<p>Following which you should get the following output:</p>
<pre><code>Unable to find image &#39;hello-world:latest&#39; locally
latest: Pulling from library/hello-world
0e03bdcc26d7: Pull complete 
Digest: sha256:6a65f928fb91fcfbc963f7aa6d57c8eeb426ad9a20c7ee045538ef34847f44f1
Status: Downloaded newer image for hello-world:latest

Hello from Docker!
This message shows that your installation appears to be working correctly.

To generate this message, Docker took the following steps:
 1. The Docker client contacted the Docker daemon.
 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub.
    (amd64)
 3. The Docker daemon created a new container from that image which runs the
    executable that produces the output you are currently reading.
 4. The Docker daemon streamed that output to the Docker client, which sent it
    to your terminal.

To try something more ambitious, you can run an Ubuntu container with:
 $ docker run -it ubuntu bash

Share images, automate workflows, and more with a free Docker ID:
 https://hub.docker.com/

For more examples and ideas, visit:
 https://docs.docker.com/get-started/
</code></pre>
</div>
<div id="installing-rselenium" class="section level2">
<h2>Installing RSelenium</h2>
<p>You can install and load RSelenium in the way you’re used to:</p>
<pre class="r"><code>install.packages(&#39;RSelenium&#39;)</code></pre>
<p>It seems that <code>devtools</code> is required to get Selenium working, so load it with:</p>
<pre class="r"><code>library(devtools)
library(RSelenium)</code></pre>
<p>Now you can start up a Selenium-server (in command prompt) by entering:</p>
<pre><code>$ sudo docker run -d -p 4445:4444 selenium/standalone-chrome</code></pre>
<p>Check if it’s working by entering <code>docker ps</code> in command prompt, following which you should get a list containing the one server you just set up, and some additional information (CONTAINER ID, IMAGE, etc.)</p>
<p>You can replace chrome by any other browser, and the version no. by a specific desired version number of your liking, by entering e.g. <code>$ sudo docker run -d -p 4445:4444 selenium/standalone-chrone:1.0.0</code> Now, back in R, we use <code>RSelenium</code> to connect to the server we’ve just created:</p>
<pre class="r"><code>remDr &lt;- RSelenium::remoteDriver(remoteServerAddr = &quot;localhost&quot;,
                                 port = 4445L,
                                 browserName = &quot;chrome&quot;)</code></pre>
<p>And we can start navigating, just like we would do in a real browser:</p>
<pre class="r"><code>remDr$open()

remDr$navigate(&quot;http://bas-m.netlify.app&quot;) </code></pre>
<p>The only difference is that we know explicitly write down our activities (clicks, filling in forms, etc.) using a few simple commands. The basic commands used to navigate a browser can be found <a href="https://cran.r-project.org/web/packages/RSelenium/vignettes/basics.html">here</a>, for example.</p>
</div>
<div id="scraping-in-r-using-selenium" class="section level2">
<h2>Scraping in R Using Selenium</h2>
<p>Now, let’s get back to the job we started with: scraping the names of Iranian MP’s! We know how to navigate to the desired webpage in RSelenium, and we know we want the following:</p>
<ol style="list-style-type: decimal">
<li>We want to scrape all MP’s names on page 1.</li>
<li>We want to go to page 2.</li>
<li>We want to scrape all MP’s names on page 2.</li>
<li>Etc. etc. until page 29</li>
</ol>
<p>Let us start with the sample case - we scrape all names from page 1, and then click on page 2, and scrape all names from that page:</p>
<pre class="r"><code>#By default, we end up at page 1
remDr$navigate(&quot;https://en.parliran.ir/eng/en/10th%20Term&quot;)

remDr$getPageSource() %&gt;%
  magrittr::extract2(1) %&gt;%
  read_html() %&gt;%
  html_nodes(&quot;div .text-left.h4:not(.blue)&quot;) %&gt;%
  html_text()</code></pre>
<pre><code>##  [1] &quot;Name:Abasi, Asadollah&quot;             &quot;Name:Abedi, Heydar Ali&quot;           
##  [3] &quot;Name:Abramian, Jorjik&quot;             &quot;Name:Abtahi, seyed Mohammad Javad&quot;
##  [5] &quot;Name:Abutorabi, Abolfazl&quot;          &quot;Name:Adyani rad, Seyed Ali&quot;       
##  [7] &quot;Name:Afzali, Nazar&quot;                &quot;Name:Afzali, Seyed Hossein&quot;       
##  [9] &quot;Name:Aghapour Alishahi, Masoumeh&quot;  &quot;Name:Ahmadi Lashki, Qasem&quot;</code></pre>
<p>Notice that we used the same CSS Selector as before to get the names. If all pages have a similar markup, then we can use the same CSS Selector on every page. We’ll soon find out whether that’s the case. Let us store the obtained names in a vector:</p>
<pre class="r"><code>names &lt;- remDr$getPageSource() %&gt;%
  magrittr::extract2(1) %&gt;%
  read_html() %&gt;%
  html_nodes(&quot;div .text-left.h4:not(.blue)&quot;) %&gt;%
  html_text()</code></pre>
<p>.. and attempt to navigate to page two:</p>
<pre class="r"><code>remDr$navigate(&quot;https://en.parliran.ir/eng/en/10th%20Term&quot;)

remDr$findElement(using = &quot;css&quot;, &#39;input[value=&quot;2&quot;]&#39;) -&gt; elem
elem$clickElement()</code></pre>
<p>Now we have navigated to page 2! Let us now find the names on page 2:</p>
<pre class="r"><code>remDr$getPageSource() %&gt;%
  magrittr::extract2(1) %&gt;%
  read_html() %&gt;%
  html_nodes(&quot;div .text-left.h4:not(.blue)&quot;) %&gt;%
  html_text()</code></pre>
<pre><code>##  [1] &quot;Name:Ahmadi, Fereydoun&quot;       &quot;Name:Akbari, Ali&quot;            
##  [3] &quot;Name:Akbari, Ali(Shiraz)&quot;     &quot;Name:Akbariyan, Aziz&quot;        
##  [5] &quot;Name:Alavi, Seyed Ahsan&quot;      &quot;Name:Alavi, Seyed Mohsen&quot;    
##  [7] &quot;Name:Alijani Zamani, Mohsen&quot;  &quot;Name:Alireza beygi, Ahmad&quot;   
##  [9] &quot;Name:Alizade, Reza&quot;           &quot;Name:Allah gholi zade, Gholi&quot;</code></pre>
<p>And let’s also store them in <code>names</code>, the vector we’ve made before:</p>
<pre class="r"><code>names &lt;- c(names, 
           remDr$getPageSource() %&gt;%
                magrittr::extract2(1) %&gt;%
                read_html() %&gt;%
                html_nodes(&quot;div .text-left.h4:not(.blue)&quot;) %&gt;%
                html_text()
)

names</code></pre>
<pre><code>##  [1] &quot;Name:Abasi, Asadollah&quot;             &quot;Name:Abedi, Heydar Ali&quot;           
##  [3] &quot;Name:Abramian, Jorjik&quot;             &quot;Name:Abtahi, seyed Mohammad Javad&quot;
##  [5] &quot;Name:Abutorabi, Abolfazl&quot;          &quot;Name:Adyani rad, Seyed Ali&quot;       
##  [7] &quot;Name:Afzali, Nazar&quot;                &quot;Name:Afzali, Seyed Hossein&quot;       
##  [9] &quot;Name:Aghapour Alishahi, Masoumeh&quot;  &quot;Name:Ahmadi Lashki, Qasem&quot;        
## [11] &quot;Name:Ahmadi, Fereydoun&quot;            &quot;Name:Akbari, Ali&quot;                 
## [13] &quot;Name:Akbari, Ali(Shiraz)&quot;          &quot;Name:Akbariyan, Aziz&quot;             
## [15] &quot;Name:Alavi, Seyed Ahsan&quot;           &quot;Name:Alavi, Seyed Mohsen&quot;         
## [17] &quot;Name:Alijani Zamani, Mohsen&quot;       &quot;Name:Alireza beygi, Ahmad&quot;        
## [19] &quot;Name:Alizade, Reza&quot;                &quot;Name:Allah gholi zade, Gholi&quot;</code></pre>
</div>
<div id="scraping-all-pages" class="section level2">
<h2>Scraping all pages</h2>
<p>Let us now proceed to scrape all pages at once, meaning, using one <code>for</code> loop! We can make clever use that the values of the CSS selectors for the buttons are numbers 1 to 29, so we can use that to generate code to go to the next page after storing the results of the current page. For clarity’s sake, we start from scratch, closing our current session:</p>
<pre class="r"><code>remDr$close()</code></pre>
<p>And assuming only that we’ve set up our Selenium server using Docker:</p>
<pre class="r"><code>remDr &lt;- RSelenium::remoteDriver(remoteServerAddr = &quot;localhost&quot;,
                                 port = 4445L,
                                 browserName = &quot;chrome&quot;)

remDr$open()

remDr$navigate(&quot;https://en.parliran.ir/eng/en/10th%20Term&quot;)

#Page 1
names &lt;- remDr$getPageSource() %&gt;%
  magrittr::extract2(1) %&gt;%
  read_html() %&gt;%
  html_nodes(&quot;div .text-left.h4:not(.blue)&quot;) %&gt;%
  html_text()

#Page 2 to 29
for(i in 2:29) {
  #Get the correct CSS Selector for each page
  element &lt;- paste(&#39;input[value=&quot;x&quot;]&#39;)
  element &lt;- stringr::str_replace(element, &quot;x&quot;, as.character(i))
  
  #Click on page i button
  remDr$findElement(using = &quot;css&quot;, element) -&gt; clickthis
  clickthis$clickElement()
  
  #Scrape the names
  names &lt;- c(names, 
             remDr$getPageSource() %&gt;%
               magrittr::extract2(1) %&gt;%
               read_html() %&gt;%
               html_nodes(&quot;div .text-left.h4:not(.blue)&quot;) %&gt;%
               html_text()
  )
  
  names
  
}</code></pre>
<p>And here we have it! 286 names!</p>
<pre class="r"><code>head(names)</code></pre>
<pre><code>## [1] &quot;Name:Abasi, Asadollah&quot;             &quot;Name:Abedi, Heydar Ali&quot;           
## [3] &quot;Name:Abramian, Jorjik&quot;             &quot;Name:Abtahi, seyed Mohammad Javad&quot;
## [5] &quot;Name:Abutorabi, Abolfazl&quot;          &quot;Name:Adyani rad, Seyed Ali&quot;</code></pre>
<p>Let’s clean them quickly, for completness’ sake:</p>
<pre class="r"><code>names %&gt;%
  stringr::str_replace(&quot;Name:&quot;, &quot;&quot;) %&gt;%
  tail()</code></pre>
<pre><code>## [1] &quot;Hashemi, Homayoun&quot;                  &quot;Hashemzaei, Abdolreza&quot;             
## [3] &quot;Hassan Beygi, Abolfazl&quot;             &quot;Hassan nejad, Mohammad&quot;            
## [5] &quot;Hassani Jouriyabi, Mohammad Sadegh&quot; &quot;Hassanpour Biglari, Shahbaz&quot;</code></pre>
</div>
<div id="final-showcase-capturing-names-and-biographical-information" class="section level2">
<h2>Final Showcase: Capturing Names and Biographical Information</h2>
<p>So far the small tutorial, of which I hope it was easy enough for any readers to follow along. Now, I want to illustrate some more serious applications: some of you might have noticed that the names also contain links to personalized pages of the MP’s, which in turn contains <code>&lt;div&gt;</code>’s with personal (biographical) information. I want to do this in 3 steps:</p>
<ol style="list-style-type: decimal">
<li>I gather all the links of the MP’s over the 10 pages.</li>
<li>I go to each of those 286 links and extract the information</li>
<li>I clean the data obtained in step 2.</li>
</ol>
</div>
<div id="step-1-gather-all-links" class="section level2">
<h2>Step 1: Gather all links</h2>
<p>In this step, I use <code>RSelenium</code> to browse through each of the ten pages, and extract the hyperlink for each politician on a particular page (check out the CSS Selector that I use).</p>
<pre class="r"><code>remDr$navigate(&quot;https://en.parliran.ir/eng/en/10th%20Term&quot;)
links &lt;- NULL

for(i in 1:29){
  
  #Get the correct CSS Selector for each page
  element &lt;- paste(&#39;input[value=&quot;x&quot;]&#39;)
  element &lt;- stringr::str_replace(element, &quot;x&quot;, as.character(i))
  
  #Click on page i button
  remDr$findElement(using = &quot;css&quot;, element) -&gt; clickthis
  clickthis$clickElement()
  
  
  #Extract all the hyperlinks to politician&#39;s pages on page i
  links &lt;- c(links, 
             remDr$getPageSource() %&gt;%
                magrittr::extract2(1) %&gt;%
                read_html() %&gt;%
                html_nodes(&#39;div.col-lg-10 a[target=&quot;_blank&quot;]&#39;) %&gt;%
                html_attr(&quot;href&quot;)
  )
  
  links
  
}</code></pre>
<p>And there they are!</p>
<pre class="r"><code>head(links)</code></pre>
<pre><code>## [1] &quot;https://en.parliran.ir/eng/en/Content/_/Abasi-Asadollah17929&quot;            
## [2] &quot;https://en.parliran.ir/eng/en/Content/_/Abedi-Heydar-Ali19627&quot;           
## [3] &quot;https://en.parliran.ir/eng/en/Content/_/Abramian-Jorjik21155&quot;            
## [4] &quot;https://en.parliran.ir/eng/en/Content/_/Abtahi-seyed-Mohammad-Javad22550&quot;
## [5] &quot;https://en.parliran.ir/eng/en/Content/_/Abutorabi-Abolfazl23678&quot;         
## [6] &quot;https://en.parliran.ir/eng/en/Content/_/Adyani-rad-Seyed-Ali25134&quot;</code></pre>
</div>
<div id="step-2-go-to-the-links-and-extract-information" class="section level2">
<h2>Step 2: Go to the Links and Extract Information</h2>
<p>In this step, I will go through each of the 286 obtained links, and extract the necessary information.</p>
<pre class="r"><code>info &lt;- list()
for(i in 1:length(links)){
  print(i)
  info[[i]] &lt;- read_html(links[i]) %&gt;%
    html_nodes(&quot;div.col-lg-7 p&quot;) %&gt;%
    html_text() %&gt;%
    stringr::str_trim(side = &quot;both&quot;) %&gt;%
    .[. != &quot;&quot;] 
}</code></pre>
<pre><code>## [1] 1
## [1] 2
## [1] 3
## [1] 4
## [1] 5
## [1] 6
## [1] 7
## [1] 8
## [1] 9
## [1] 10
## [1] 11
## [1] 12
## [1] 13
## [1] 14
## [1] 15
## [1] 16
## [1] 17
## [1] 18
## [1] 19
## [1] 20
## [1] 21
## [1] 22
## [1] 23
## [1] 24
## [1] 25
## [1] 26
## [1] 27
## [1] 28
## [1] 29
## [1] 30
## [1] 31
## [1] 32
## [1] 33
## [1] 34
## [1] 35
## [1] 36
## [1] 37
## [1] 38
## [1] 39
## [1] 40
## [1] 41
## [1] 42
## [1] 43
## [1] 44
## [1] 45
## [1] 46
## [1] 47
## [1] 48
## [1] 49
## [1] 50
## [1] 51
## [1] 52
## [1] 53
## [1] 54
## [1] 55
## [1] 56
## [1] 57
## [1] 58
## [1] 59
## [1] 60
## [1] 61
## [1] 62
## [1] 63
## [1] 64
## [1] 65
## [1] 66
## [1] 67
## [1] 68
## [1] 69
## [1] 70
## [1] 71
## [1] 72
## [1] 73
## [1] 74
## [1] 75
## [1] 76
## [1] 77
## [1] 78
## [1] 79
## [1] 80
## [1] 81
## [1] 82
## [1] 83
## [1] 84
## [1] 85
## [1] 86
## [1] 87
## [1] 88
## [1] 89
## [1] 90
## [1] 91
## [1] 92
## [1] 93
## [1] 94
## [1] 95
## [1] 96
## [1] 97
## [1] 98
## [1] 99
## [1] 100
## [1] 101
## [1] 102
## [1] 103
## [1] 104
## [1] 105
## [1] 106
## [1] 107
## [1] 108
## [1] 109
## [1] 110
## [1] 111
## [1] 112
## [1] 113
## [1] 114
## [1] 115
## [1] 116
## [1] 117
## [1] 118
## [1] 119
## [1] 120
## [1] 121
## [1] 122
## [1] 123
## [1] 124
## [1] 125
## [1] 126
## [1] 127
## [1] 128
## [1] 129
## [1] 130
## [1] 131
## [1] 132
## [1] 133
## [1] 134
## [1] 135
## [1] 136
## [1] 137
## [1] 138
## [1] 139
## [1] 140
## [1] 141
## [1] 142
## [1] 143
## [1] 144
## [1] 145
## [1] 146
## [1] 147
## [1] 148
## [1] 149
## [1] 150
## [1] 151
## [1] 152
## [1] 153
## [1] 154
## [1] 155
## [1] 156
## [1] 157
## [1] 158
## [1] 159
## [1] 160
## [1] 161
## [1] 162
## [1] 163
## [1] 164
## [1] 165
## [1] 166
## [1] 167
## [1] 168
## [1] 169
## [1] 170
## [1] 171
## [1] 172
## [1] 173
## [1] 174
## [1] 175
## [1] 176
## [1] 177
## [1] 178
## [1] 179
## [1] 180
## [1] 181
## [1] 182
## [1] 183
## [1] 184
## [1] 185
## [1] 186
## [1] 187
## [1] 188
## [1] 189
## [1] 190
## [1] 191
## [1] 192
## [1] 193
## [1] 194
## [1] 195
## [1] 196
## [1] 197
## [1] 198
## [1] 199
## [1] 200
## [1] 201
## [1] 202
## [1] 203
## [1] 204
## [1] 205
## [1] 206
## [1] 207
## [1] 208
## [1] 209
## [1] 210
## [1] 211
## [1] 212
## [1] 213
## [1] 214
## [1] 215
## [1] 216
## [1] 217
## [1] 218
## [1] 219
## [1] 220
## [1] 221
## [1] 222
## [1] 223
## [1] 224
## [1] 225
## [1] 226
## [1] 227
## [1] 228
## [1] 229
## [1] 230
## [1] 231
## [1] 232
## [1] 233
## [1] 234
## [1] 235
## [1] 236
## [1] 237
## [1] 238
## [1] 239
## [1] 240
## [1] 241
## [1] 242
## [1] 243
## [1] 244
## [1] 245
## [1] 246
## [1] 247
## [1] 248
## [1] 249
## [1] 250
## [1] 251
## [1] 252
## [1] 253
## [1] 254
## [1] 255
## [1] 256
## [1] 257
## [1] 258
## [1] 259
## [1] 260
## [1] 261
## [1] 262
## [1] 263
## [1] 264
## [1] 265
## [1] 266
## [1] 267
## [1] 268
## [1] 269
## [1] 270
## [1] 271
## [1] 272
## [1] 273
## [1] 274
## [1] 275
## [1] 276
## [1] 277
## [1] 278
## [1] 279
## [1] 280
## [1] 281
## [1] 282
## [1] 283
## [1] 284
## [1] 285
## [1] 286</code></pre>
</div>
<div id="step-3-cleaning-the-data" class="section level2">
<h2>Step 3: Cleaning the Data</h2>
<p>Finally, I can do some cleaning exercises, to make sure the data can be used easily by myself and others. You can download the obtained data <a href="/iranianmps.csv">here</a>!</p>
<pre class="r"><code>iranianmps &lt;- lapply(info, data.frame) %&gt;%
  lapply(setNames, &quot;var&quot;) %&gt;%
  lapply(tidyr::separate, var, sep = &quot;:&quot;, into = c(&quot;var&quot;, &quot;val&quot;)) %&gt;%
  lapply(tidyr::pivot_wider, names_from = &quot;var&quot;, values_from = &quot;val&quot;) %&gt;%
  purrr::reduce(dplyr::bind_rows)</code></pre>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>In this blog post, I demonstrated how to install and use RSelenium in the context of scraping information about Iranian Members of Parliament. I hope it was useful to some of you aiming to learn RSelenium, and tears down some barriers to first usage. Anyway, after you’re done, you can close the connection with the Selenium server in R with:</p>
<pre class="r"><code>remDr$close()</code></pre>
<p>And if you want to shut down your Selenium server in Docker, go to command prompt:</p>
<pre><code>$ docker kill [CONTAINER_ID]</code></pre>
<p>where [CONTAINER_ID] is a string corresponding to the CONTAINDER_ID box of your server. Thank you for reading, and if you have any questions, you can always <a href="mailto:a.h.machielsen@uu.nl">contact me</a>.</p>
</div>
